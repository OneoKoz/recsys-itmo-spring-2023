{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CrjZ-Ma78a0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch_lightning\n",
      "  Downloading pytorch_lightning-2.0.1.post0-py3-none-any.whl (718 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m718.6/718.6 kB\u001B[0m \u001B[31m10.5 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:04\u001B[0m\n",
      "\u001B[?25hCollecting lightning-utilities>=0.7.0\n",
      "  Downloading lightning_utilities-0.8.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /Users/ivan/Desktop/recsys-itmo-spring-2023/venv/lib/python3.10/site-packages (from pytorch_lightning) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /Users/ivan/Desktop/recsys-itmo-spring-2023/venv/lib/python3.10/site-packages (from pytorch_lightning) (4.62.2)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /Users/ivan/Desktop/recsys-itmo-spring-2023/venv/lib/python3.10/site-packages (from pytorch_lightning) (1.24.2)\n",
      "Collecting torchmetrics>=0.7.0\n",
      "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m519.2/519.2 kB\u001B[0m \u001B[31m13.1 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:03\u001B[0m\n",
      "\u001B[?25hCollecting typing-extensions>=4.0.0\n",
      "  Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: packaging>=17.1 in /Users/ivan/Desktop/recsys-itmo-spring-2023/venv/lib/python3.10/site-packages (from pytorch_lightning) (23.0)\n",
      "Collecting fsspec[http]>2021.06.0\n",
      "  Downloading fsspec-2023.4.0-py3-none-any.whl (153 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m154.0/154.0 kB\u001B[0m \u001B[31m11.9 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hCollecting torch>=1.11.0\n",
      "  Downloading torch-2.0.0-cp310-none-macosx_10_9_x86_64.whl (139.8 MB)\n",
      "\u001B[2K     \u001B[91m━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.2/139.8 MB\u001B[0m \u001B[31m61.0 kB/s\u001B[0m eta \u001B[36m0:36:32\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip install pytorch_lightning\n",
    "!pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1CmEukeg8Njd"
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as td\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import tqdm\n",
    "import json\n",
    "import sklearn.metrics as sm\n",
    "\n",
    "import tensorboardX as tb\n",
    "import tensorflow as tf\n",
    "import datetime, os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(31337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: /Users/ivan/Desktop/recsys-itmo-spring-2023/venv/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "!whereis python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4cLf0zW8Njf"
   },
   "source": [
    "## Create pairs (first track, subsequent track, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "DHUIFjU0Z09C"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MKlgAqq-8Njg"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"/content/drive/MyDrive/recsys-itmo-2023/seminar_05/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y9aeehkP8Njh"
   },
   "outputs": [],
   "source": [
    "data = pd.read_json(DATA_DIR + \"data.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zj9JftT88Njh"
   },
   "outputs": [],
   "source": [
    "Pair = namedtuple(\"Session\", [\"user\", \"start\", \"track\", \"time\"])\n",
    "\n",
    "def get_pairs(user_data):\n",
    "    pairs = []\n",
    "    first = None\n",
    "    for _, row in user_data.sort_values(\"timestamp\").iterrows():\n",
    "        if first is None:\n",
    "            first = row[\"track\"]\n",
    "        else:\n",
    "            pairs.append(Pair(row[\"user\"], first, row[\"track\"], row[\"time\"]))\n",
    "        \n",
    "        if row[\"message\"] == \"last\":\n",
    "            first = None\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4c_Ifi9_8Nji"
   },
   "outputs": [],
   "source": [
    "pairs = pd.DataFrame(\n",
    "    data\n",
    "    .groupby(\"user\")\n",
    "    .apply(get_pairs)\n",
    "    .explode()\n",
    "    .values\n",
    "    .tolist(),\n",
    "    columns=[\"user\", \"start\", \"track\", \"time\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eA0LzG3Z8Nji"
   },
   "outputs": [],
   "source": [
    "figure, ax = plt.subplots()\n",
    "sns.histplot(pairs[\"time\"], ax=ax)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PkYDflFK8Njj"
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cE63YQAi8Njj"
   },
   "outputs": [],
   "source": [
    "rdm = np.random.random(len(pairs))\n",
    "train_data = pairs[rdm < 0.8]\n",
    "val_data = pairs[(rdm >= 0.8) & (rdm < 0.9)]\n",
    "test_data = pairs[rdm >= 0.9]\n",
    "\n",
    "len(train_data), len(val_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2N72w3Ym8Njl"
   },
   "outputs": [],
   "source": [
    "class ContextualRanker(pl.LightningModule):\n",
    "    def __init__(self, embedding_dim=10):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # We won't have embeddings for everything, but that's ok\n",
    "        self.context = nn.Embedding(num_embeddings=50000, embedding_dim=self.embedding_dim)\n",
    "        self.track = nn.Embedding(num_embeddings=50000, embedding_dim=self.embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        context = self.context(x[:, 0]) # start track\n",
    "        track = self.track(x[:, 1]) # next track\n",
    "        return torch.sum(context * track, dim=1)\n",
    "            \n",
    "    def step(self, batch, batch_idx, metric, prog_bar=False):\n",
    "        x, y = batch\n",
    "        predictions = self.forward(x)\n",
    "        loss = F.mse_loss(predictions, y.float(), reduction='mean')\n",
    "        self.log(metric, loss, prog_bar=prog_bar)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx, prog_bar=False):\n",
    "        x, y = batch\n",
    "        predictions = self.forward(x)\n",
    "        targets = y[:, 0].float()\n",
    "        avgs = y[:, 1].float()\n",
    "        rdms = y[:, 2].float()\n",
    "\n",
    "        loss = F.mse_loss(predictions, targets, reduction='mean')\n",
    "        avg_loss = F.mse_loss(avgs, targets, reduction='mean')\n",
    "        rdm_loss = F.mse_loss(rdms, targets, reduction='mean')\n",
    "\n",
    "        self.log(\"test_loss\", loss, prog_bar=prog_bar)\n",
    "        self.log(\"avg_loss\", avg_loss, prog_bar=prog_bar)\n",
    "        self.log(\"rdm_loss\", rdm_loss, prog_bar=prog_bar)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step(batch, batch_idx, \"train_loss\")\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step(batch, batch_idx, \"val_loss\", True)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True)\n",
    "        scheduler = {\n",
    "            'scheduler': lr_scheduler,\n",
    "            'reduce_on_plateau': True,\n",
    "            'monitor': 'val_loss'\n",
    "        }\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XSZTEW7h9d3p"
   },
   "outputs": [],
   "source": [
    "class ContextualRankerData(pl.LightningDataModule):\n",
    "  def __init__(self, train_data, val_data, test_data, features):\n",
    "      super().__init__()\n",
    "      self.train_data = train_data\n",
    "      self.val_data = val_data\n",
    "      self.test_data = test_data\n",
    "      self.features = features\n",
    "\n",
    "  def prepare_data(self):\n",
    "      self.test_data = self.test_data.assign(rdm = np.random.random(len(self.test_data))).assign(avg = self.train_data[\"time\"].mean())\n",
    "\n",
    "  def setup(self, stage=None):\n",
    "      if stage == \"fit\" or stage is None:\n",
    "        self.train_dataset = td.TensorDataset(\n",
    "            torch.from_numpy(self.train_data[self.features].values), \n",
    "            torch.from_numpy(self.train_data[\"time\"].values)\n",
    "            )\n",
    "\n",
    "        self.val_dataset = td.TensorDataset(\n",
    "            torch.from_numpy(self.val_data[self.features].values), \n",
    "            torch.from_numpy(self.val_data[\"time\"].values)\n",
    "            )\n",
    "        \n",
    "      if stage == \"test\" or stage is None:  \n",
    "        self.test_dataset = td.TensorDataset(\n",
    "            torch.from_numpy(self.test_data[self.features].values),\n",
    "            torch.from_numpy(self.test_data[[\"time\", \"avg\", \"rdm\"]].values)\n",
    "        )\n",
    "  def train_dataloader(self):\n",
    "      return td.DataLoader(self.train_dataset, batch_size=2048, shuffle=True, num_workers=0)\n",
    "\n",
    "  def val_dataloader(self):\n",
    "      return td.DataLoader(self.val_dataset, batch_size=2048, num_workers=0)\n",
    "\n",
    "  def test_dataloader(self):\n",
    "      return td.DataLoader(self.test_dataset, batch_size=512, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JWZ8cqTZ8Njm"
   },
   "outputs": [],
   "source": [
    "net = ContextualRanker(embedding_dim=100)\n",
    "data_module = ContextualRankerData(train_data, val_data, test_data, features = [\"start\", \"track\"])\n",
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(monitor=\"val_loss\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=300,\n",
    "    accelerator='gpu', \n",
    "    devices=1,\n",
    "    callbacks=[\n",
    "        pl.callbacks.early_stopping.EarlyStopping(monitor=\"val_loss\", patience=5),\n",
    "        pl.callbacks.LearningRateMonitor(logging_interval=\"step\"),\n",
    "        checkpoint_callback\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "omCmoxVhGfJ2"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /content/lightning_logs --host localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sqy8qDr98Njm"
   },
   "outputs": [],
   "source": [
    "trainer.fit(\n",
    "    net, \n",
    "    data_module\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_IeB7jzb8Njn"
   },
   "outputs": [],
   "source": [
    "best = ContextualRanker.load_from_checkpoint(checkpoint_callback.best_model_path, embedding_dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HTUgc8_hQ7N0"
   },
   "outputs": [],
   "source": [
    "trainer.test(best, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRZLUR9_8Njo"
   },
   "source": [
    "## Compute top recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5UrLSjgt8Njo"
   },
   "outputs": [],
   "source": [
    "track_meta = pd.read_json(DATA_DIR + \"tracks.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PqwVsVyO8Njp"
   },
   "outputs": [],
   "source": [
    "context_embeddings = dict(best.named_parameters())[\"context.weight\"].data.cpu().numpy()\n",
    "track_embeddings = dict(best.named_parameters())[\"track.weight\"].data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-z1VTPGh8Njp"
   },
   "outputs": [],
   "source": [
    "track_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m7_A20no8Njp"
   },
   "outputs": [],
   "source": [
    "k = 100\n",
    "with open(DATA_DIR + \"tracks_with_recs.json\", \"w\") as rf:\n",
    "    for _, track in tqdm.tqdm(track_meta.iterrows()):\n",
    "        embedding = context_embeddings[track[\"track\"]]\n",
    "        neighbours = np.argpartition(-np.dot(track_embeddings, embedding), k)[:k]\n",
    "        \n",
    "        recommendation = dict(track)\n",
    "        recommendation[\"recommendations\"] = neighbours.tolist()\n",
    "        \n",
    "        rf.write(json.dumps(recommendation) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QhS_o_ff8Njp"
   },
   "outputs": [],
   "source": [
    "track = 3916\n",
    "embedding = context_embeddings[track]\n",
    "track_meta.loc[track_meta[\"track\"] == track, [\"artist\", \"title\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gGFHEN8g8Njq"
   },
   "outputs": [],
   "source": [
    "k = 10\n",
    "neighbours = np.argpartition(-np.dot(track_embeddings, embedding), k)[:k]\n",
    "track_meta.loc[track_meta[\"track\"].isin(neighbours), [\"artist\", \"title\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ymCblQht8Njq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}